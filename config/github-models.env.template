# GitHub Models Configuration Template
# ====================================
# Copy this file to github-models.env and fill in your values

# Authentication
GITHUB_TOKEN=your-github-token
GITHUB_MODELS_ENDPOINT=https://models.inference.ai.azure.com
GITHUB_MODELS_API_VERSION=2024-04-01-preview

# Model Configuration
GITHUB_PRIMARY_MODEL=claude-3-5-sonnet
GITHUB_SECONDARY_MODEL=claude-3-5-haiku
GITHUB_FALLBACK_MODELS=gpt-4o,gpt-4o-mini,claude-3-opus

# Proxy Configuration
GITHUB_PROXY_PORT=8082
GITHUB_PROXY_HOST=0.0.0.0
LITELLM_PORT=8083

# Rate Limiting (GitHub Models limits)
GITHUB_MAX_REQUESTS_PER_MINUTE=500
GITHUB_MAX_TOKENS_PER_MINUTE=100000
GITHUB_MAX_CONCURRENT_REQUESTS=5

# Cost Configuration (GitHub Models pricing)
GITHUB_INPUT_COST_PER_TOKEN=0.0000025
GITHUB_OUTPUT_COST_PER_TOKEN=0.00001
GITHUB_COST_MULTIPLIER=0.8

# Performance Settings
GITHUB_REQUEST_TIMEOUT=180
GITHUB_MAX_RETRIES=3
GITHUB_RETRY_DELAY=2.0

# Health Check
GITHUB_HEALTH_CHECK_INTERVAL=60
GITHUB_HEALTH_CHECK_TIMEOUT=10

# liteLLM Configuration
LITELLM_LOG_LEVEL=INFO
LITELLM_ENABLE_CACHING=true
LITELLM_CACHE_TTL=300

# Logging
GITHUB_LOG_LEVEL=INFO
GITHUB_ENABLE_REQUEST_LOGGING=true